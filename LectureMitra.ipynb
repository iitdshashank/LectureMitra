{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7YAntiCUywy"
      },
      "source": [
        "# LectureMitra: Your Conversational YouTube Tutor\n",
        "\n",
        "\n",
        "Welcome to LectureMitra! This notebook contains a complete, voice-enabled AI tutor that can help you study any YouTube video.\n",
        "\n",
        "How it Works:\n",
        "Input a URL: You provide a link to any YouTube video that has English captions.\n",
        "Build a Brain: The app fetches the transcript and builds a temporary \"knowledge base\" or \"brain\" for that video.\n",
        "Ask Anything: You can ask questions using text or your voice.\n",
        "Get Answers: The tutor answers your questions using both text and voice, based only on the content of the video you provided.\n",
        "\n",
        "-----------------------------------------------------------\n",
        "\n",
        "Instructions for Use:\n",
        "To start the application, simply run all the cells in order.\n",
        "\n",
        "\n",
        "Go to the menu and click Runtime -> Run all.\n",
        "\n",
        "When a box appears asking for an API key, paste in your Sarvam AI API key and press Enter.\n",
        "\n",
        "Scroll to the bottom of the notebook. An input box will appear asking for a YouTube video URL. Paste the link and press Enter to begin your session!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**For any doubt while running the code, take reference from the demo video. ( https://youtu.be/EXim3NoRhRI )**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzOFfEIkVeKP"
      },
      "source": [
        "# Step 0: Installing Dependencies\n",
        "\n",
        "This first cell installs all the necessary Python libraries required for the project.\n",
        "\n",
        "youtube-transcript-api: To fetch captions directly from YouTube.\n",
        "\n",
        "langchain, faiss-cpu, sentence-transformers: These form the core of our Retrieval-Augmented Generation (RAG) system. They help in splitting the text, creating numerical embeddings, and building a searchable vector database.\n",
        "\n",
        "requests: A standard library for making API calls to Sarvam AI.\n",
        "\n",
        "ipywidgets, IPython: Utilities for handling audio recording and playback within this Colab notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rujDGAcCjxnn"
      },
      "outputs": [],
      "source": [
        "print(\"Installing necessary libraries...\")\n",
        "\n",
        "# For YouTube transcripts\n",
        "#pip install --upgrade youtube-transcript-api -q\n",
        "!pip install yt-dlp -q\n",
        "\n",
        "# For the RAG pipeline (vector store, embeddings, text splitting)\n",
        "!pip install langchain langchain-community faiss-cpu sentence-transformers -q\n",
        "\n",
        "# For making API calls to Sarvam\n",
        "!pip install requests -q\n",
        "\n",
        "# For audio recording and playback in Colab\n",
        "!pip install ipywidgets IPython -q\n",
        "\n",
        "print(\"Installations complete!\")\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import requests\n",
        "from getpass import getpass\n",
        "from IPython.display import display, Javascript, Audio\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import io\n",
        "\n",
        "# Securely get your Sarvam API Key\n",
        "# When you run this cell, it will prompt you to enter your key.\n",
        "# This is much safer than pasting it directly into the code.\n",
        "SARVAM_API_KEY = getpass('Please enter your Sarvam AI API Key: ')\n",
        "os.environ['SARVAM_API_KEY'] = SARVAM_API_KEY\n",
        "print(\"API Key has been set up successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWFBzuz67AbC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iameMdGJfij2"
      },
      "source": [
        "# Step 0.5 (Optional) Professional Logging Setup\n",
        "\n",
        "\n",
        "Before we define our main functions, we'll set up a professional logger. This is better than using print() because it provides structured, timestamped output with different severity levels (e.g., INFO, ERROR). This makes debugging much more efficient. This logger will be used by all subsequent functions to report their status."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcrVqoiUcMwZ"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "\n",
        "# Configure the logger\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO, # Set the minimum level of messages to display\n",
        "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
        "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
        ")\n",
        "\n",
        "# Create a logger instance\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "logger.info(\"Logger configured successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_49HJVvcMay"
      },
      "source": [
        "# Step 1: Fetching the YouTube Transcript\n",
        "\n",
        "This is a critical function that fetches the video's transcript. This version uses the industry-standard yt-dlp library, which is highly robust against changes in YouTube's website structure.\n",
        "\n",
        "\n",
        "It works by:\n",
        "\n",
        "\n",
        "Using a command-line call to yt-dlp to download only the English subtitle file (in .vtt format).\n",
        "\n",
        "Reading this file with Python.\n",
        "\n",
        "Parsing the file's content to extract only the clean transcript text, removing timestamps and other metadata.\n",
        "Cleaning up the downloaded file after it's been processed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6dVrp8YlsZ0"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import os\n",
        "import re\n",
        "\n",
        "def get_youtube_transcript(video_url: str) -> str | None:\n",
        "    \"\"\"\n",
        "    Fetches the transcript for a given YouTube video URL using the robust yt-dlp library.\n",
        "    Returns the transcript as a single string.\n",
        "    \"\"\"\n",
        "    logger.info(f\"Attempting to fetch transcript for {video_url} using yt-dlp.\")\n",
        "\n",
        "    # Define the output filename for the transcript\n",
        "    transcript_file = \"transcript.en.vtt\"\n",
        "\n",
        "    # Command to download the English auto-captions, skipping the video download\n",
        "    command = [\n",
        "        \"yt-dlp\",\n",
        "        \"--write-auto-sub\",       # Get auto-generated subtitles\n",
        "        \"--sub-lang\", \"en\",       # Specify English\n",
        "        \"--skip-download\",        # Don't download the video\n",
        "        \"-o\", \"transcript\",       # Base name for the output file\n",
        "        video_url\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        # Run the command. We use text=True and capture_output=True for better error logging.\n",
        "        result = subprocess.run(command, check=True, capture_output=True, text=True)\n",
        "        logger.info(\"yt-dlp command executed successfully.\")\n",
        "\n",
        "        if not os.path.exists(transcript_file):\n",
        "            logger.error(\"yt-dlp ran but the transcript file was not created. This video might not have English captions.\")\n",
        "            return None\n",
        "\n",
        "        # Read the downloaded VTT file\n",
        "        with open(transcript_file, 'r', encoding='utf-8') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        logger.info(\"Successfully read the downloaded VTT transcript file.\")\n",
        "\n",
        "        # Parse the VTT file to extract only the text\n",
        "        transcript_text_lines = []\n",
        "        for line in lines:\n",
        "            if \"-->\" not in line and \"WEBVTT\" not in line and not line.strip().isdigit() and line.strip():\n",
        "                cleaned_line = re.sub(r'<[^>]+>', '', line).strip()\n",
        "                transcript_text_lines.append(cleaned_line)\n",
        "\n",
        "        unique_lines = list(dict.fromkeys(transcript_text_lines))\n",
        "        full_transcript = \" \".join(unique_lines)\n",
        "\n",
        "        logger.info(\"Transcript parsed successfully.\")\n",
        "        return full_transcript\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        # This error happens if yt-dlp fails (e.g., video not found, no captions)\n",
        "        logger.error(f\"yt-dlp failed. Error message: {e.stderr.strip()}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        logger.error(f\"An unexpected error occurred with yt-dlp: {e}\")\n",
        "        return None\n",
        "    finally:\n",
        "        # Clean up by deleting the downloaded file\n",
        "        if os.path.exists(transcript_file):\n",
        "            os.remove(transcript_file)\n",
        "            logger.info(\"Cleaned up temporary transcript file.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwJoMTF6gE3F"
      },
      "source": [
        "# Step 2: Building the Knowledge Base (RAG Core)\n",
        "\n",
        "This is the heart of the application. The build_rag_pipeline function takes the raw transcript text and turns it into a structured, searchable knowledge base. This function will be called by our main app after a user provides a URL.\n",
        "\n",
        "This process involves:\n",
        "\n",
        "Chunking: The long transcript is broken into smaller, overlapping text chunks.\n",
        "Embedding: Each chunk is converted into a numerical vector (an \"embedding\") using a sentence-transformer model. This represents the semantic meaning of the text.\n",
        "Storing: These vectors are stored in a FAISS vector database, which allows for extremely fast similarity searches.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mM1Z3cMBmluk"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# Global variable to hold our vector database\n",
        "vector_db = None\n",
        "\n",
        "def build_rag_pipeline(transcript):\n",
        "    \"\"\"\n",
        "    Takes the transcript, chunks it, creates embeddings, and builds a vector store.\n",
        "    \"\"\"\n",
        "    global vector_db\n",
        "    if not transcript:\n",
        "        print(\"Transcript is empty. Cannot build RAG pipeline.\")\n",
        "        return\n",
        "\n",
        "    print(\"Building RAG pipeline...\")\n",
        "    # 1. Chunk the text\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=150,\n",
        "        length_function=len\n",
        "    )\n",
        "    chunks = text_splitter.split_text(transcript)\n",
        "    print(f\"Transcript split into {len(chunks)} chunks.\")\n",
        "\n",
        "    # 2. Create Embeddings\n",
        "    # We use a popular open-source model for creating the embeddings (vectors)\n",
        "    print(\"Loading embedding model...\")\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "    # 3. Create Vector Store\n",
        "    # We'll use FAISS, a fast in-memory vector store\n",
        "    print(\"Creating vector store...\")\n",
        "    vector_db = FAISS.from_texts(texts=chunks, embedding=embeddings)\n",
        "    print(\"RAG pipeline is ready!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4t8w74clgQG4"
      },
      "source": [
        "# Step 3: The Answering Engine\n",
        "\n",
        "The ask_lecturemitra function is responsible for generating an answer. It does not simply ask the AI the question. Instead, it follows the Retrieval-Augmented Generation (RAG) process:\n",
        "\n",
        "Retrieve: It takes the user's question, converts it to a vector, and searches the FAISS database to find the most relevant text chunks from the transcript.\n",
        "\n",
        "Augment: It creates a detailed prompt for the Sarvam LLM, including the user's question and the relevant chunks as \"context\".\n",
        "\n",
        "Generate: It strictly instructs the AI to answer the question only using the provided context, preventing it from making things up or using outside knowledge.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iftnRFGcp8Gc"
      },
      "outputs": [],
      "source": [
        "def ask_lecturemitra(question):\n",
        "    \"\"\"\n",
        "    Answers a question based ONLY on the transcript context.\n",
        "    This version uses the correct Sarvam Chat API endpoint and the correct model.\n",
        "    \"\"\"\n",
        "    global vector_db\n",
        "    if not vector_db:\n",
        "        return \"The RAG pipeline has not been built yet. Please provide a video link first.\"\n",
        "\n",
        "    print(f\"\\nSearching for context for the question: '{question}'\")\n",
        "\n",
        "    # 1. Retrieve relevant documents\n",
        "    retriever = vector_db.as_retriever(search_kwargs={'k': 4})\n",
        "    docs = retriever.invoke(question)\n",
        "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "    # 2. Define the prompt for the Chat model\n",
        "    system_prompt = \"\"\"You are 'LectureMitra', a helpful AI tutor. Your task is to answer the user's question STRICTLY and ONLY based on the provided 'Transcript Context'.\n",
        "    Do not use any external knowledge.\n",
        "    If the answer is not found within the context, you MUST say \"I cannot answer this question based on the provided transcript.\" \"\"\"\n",
        "\n",
        "    user_prompt_content = f\"\"\"\n",
        "    ---\n",
        "    Transcript Context:\n",
        "    {context}\n",
        "    ---\n",
        "    User's Question:\n",
        "    {question}\n",
        "    \"\"\"\n",
        "\n",
        "    # 3. Call the Sarvam LLM API\n",
        "    print(\"Asking the LLM...\")\n",
        "    try:\n",
        "        headers = {\n",
        "            \"Authorization\": f\"Bearer {os.environ['SARVAM_API_KEY']}\",\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        }\n",
        "\n",
        "        data = {\n",
        "\n",
        "            \"model\": \"sarvam-m\",\n",
        "            \"messages\": [\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt_content}\n",
        "            ],\n",
        "            \"max_tokens\": 250,\n",
        "            \"temperature\": 0.1\n",
        "        }\n",
        "\n",
        "        response = requests.post(\"https://api.sarvam.ai/v1/chat/completions\", headers=headers, json=data)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        result = response.json()\n",
        "        answer = result['choices'][0]['message']['content'].strip()\n",
        "        return answer\n",
        "\n",
        "    except requests.exceptions.HTTPError as http_err:\n",
        "        return f\"HTTP error occurred while contacting Sarvam API: {http_err} - {response.text}\"\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred: {e}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBmf-_m1gaOi"
      },
      "source": [
        "# Step 4: Enabling Voice Input (Speech-to-Text)\n",
        "\n",
        "To allow users to ask questions with their voice, we define two functions:\n",
        "\n",
        "record_audio: This uses JavaScript within Colab to access the browser's microphone and record a short audio clip.\n",
        "transcribe_audio_with_sarvam: This function takes the recorded audio file and sends it to Sarvam's Speech-to-Text (STT) API, which returns the transcribed text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J25b_kH5tkEY"
      },
      "outputs": [],
      "source": [
        "RECORD = \"\"\"\n",
        "const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\n",
        "const b2text = blob => new Promise(resolve => {\n",
        "  const reader = new FileReader()\n",
        "  reader.onloadend = e => resolve(e.srcElement.result)\n",
        "  reader.readAsDataURL(blob)\n",
        "})\n",
        "var record = time => new Promise(async resolve => {\n",
        "  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
        "  recorder = new MediaRecorder(stream)\n",
        "  chunks = []\n",
        "  recorder.ondataavailable = e => chunks.push(e.data)\n",
        "  recorder.start()\n",
        "  await sleep(time)\n",
        "  recorder.onstop = async ()=>{\n",
        "    blob = new Blob(chunks)\n",
        "    text = await b2text(blob)\n",
        "    resolve(text)\n",
        "  }\n",
        "  recorder.stop()\n",
        "})\n",
        "\"\"\"\n",
        "\n",
        "def record_audio(filename=\"recorded_audio.wav\", seconds=5):\n",
        "    \"\"\"Records audio from the browser, no changes needed here.\"\"\"\n",
        "    print(f\"Recording for {seconds} seconds... Get ready to speak!\")\n",
        "    display(Javascript(RECORD))\n",
        "    s = eval_js(f'record({seconds * 1000})')\n",
        "    b = b64decode(s.split(',')[1])\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(b)\n",
        "    print(f\"Recording finished. Audio saved as {filename}\")\n",
        "    return filename\n",
        "\n",
        "def transcribe_audio_with_sarvam(audio_filename):\n",
        "    \"\"\"\n",
        "    This function has been completely rewritten to match the STT documentation you provided.\n",
        "    \"\"\"\n",
        "    print(\"Transcribing audio with Sarvam AI (using 'saarika' model)...\")\n",
        "    api_key = os.environ.get(\"SARVAM_API_KEY\")\n",
        "    if not api_key:\n",
        "        return \"Sarvam API key not found.\"\n",
        "\n",
        "    url = \"https://api.sarvam.ai/speech-to-text\"\n",
        "    headers = {\n",
        "        \"api-subscription-key\": api_key\n",
        "    }\n",
        "\n",
        "    data = {\n",
        "\n",
        "        \"language_code\": \"en-IN\",\n",
        "        \"model\": \"saarika:v2\"\n",
        "    }\n",
        "\n",
        "    with open(audio_filename, \"rb\") as f:\n",
        "        files = {\n",
        "            'file': (audio_filename, f, 'audio/wav')\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.post(url, headers=headers, files=files, data=data)\n",
        "            response.raise_for_status()\n",
        "            transcribed_text = response.json().get(\"transcript\", \"\")\n",
        "            print(\"Transcription successful!\")\n",
        "            return transcribed_text\n",
        "        except requests.exceptions.HTTPError as http_err:\n",
        "            return f\"HTTP error during transcription: {http_err} - {response.text}\"\n",
        "        except Exception as e:\n",
        "            return f\"An error occurred during transcription: {e}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciilles8ghr3"
      },
      "source": [
        "# Step 5: Enabling Voice Output (Text-to-Speech)\n",
        "\n",
        "This function creates the voice of our tutor. It takes the text answer generated by the ask_lecturemitra function and sends it to Sarvam's Text-to-Speech (TTS) API. The API returns an audio file, which is then played back directly in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnlY0uSdxrlW"
      },
      "outputs": [],
      "source": [
        "from base64 import b64decode\n",
        "\n",
        "def generate_and_play_speech(text, language_code=\"en-IN\"):\n",
        "    \"\"\"\n",
        "    This function has been completely rewritten to match the TTS documentation you provided.\n",
        "    \"\"\"\n",
        "    if not text or \"cannot answer\" in text.lower():\n",
        "        print(\"\\nWon't generate speech for a non-answer.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\nGenerating speech with Sarvam AI (using 'bulbul' model)...\")\n",
        "    api_key = os.environ.get(\"SARVAM_API_KEY\")\n",
        "    if not api_key:\n",
        "        print(\"Sarvam API key not found.\")\n",
        "        return\n",
        "\n",
        "    url = \"https://api.sarvam.ai/text-to-speech\"\n",
        "    headers = {\n",
        "        \"api-subscription-key\": api_key,\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    data = {\n",
        "        \"text\": text,\n",
        "        \"target_language_code\": language_code,\n",
        "        \"model\": \"bulbul:v2\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(url, headers=headers, json=data)\n",
        "        response.raise_for_status()\n",
        "        response_data = response.json()\n",
        "        base64_audio = response_data['audios'][0]\n",
        "        audio_data = b64decode(base64_audio)\n",
        "\n",
        "        print(\"Speech generated successfully. Playing audio now...\")\n",
        "        display(Audio(audio_data, autoplay=True))\n",
        "\n",
        "    except requests.exceptions.HTTPError as http_err:\n",
        "        print(f\"HTTP error during speech generation: {http_err} - {response.text}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during speech generation: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pf1BqXmKgnl_"
      },
      "source": [
        "# Step 6: The Interactive Chat Session\n",
        "\n",
        "This function creates the main interactive loop of our application. It continuously prompts the user for input and manages the conversation flow. It's smart enough to handle both text input and voice commands (by calling the recording and transcription functions when the user types \"voice\"). It also includes a small time.sleep(1) delay to ensure the input prompt doesn't break after audio playback."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_bymzJ0CPMb"
      },
      "outputs": [],
      "source": [
        "def start_lecturemitra():\n",
        "    \"\"\"\n",
        "    The main orchestrator for the LectureMitra application.\n",
        "    It handles the complete user journey from URL input to Q&A.\n",
        "    \"\"\"\n",
        "    # 1. Get the YouTube URL from the user\n",
        "    video_url = input(\"Welcome to LectureMitra! Please paste the YouTube video URL you want to study: \")\n",
        "\n",
        "    if not video_url:\n",
        "        print(\"No URL provided. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # 2. Fetch the transcript for that URL\n",
        "    # We use our existing 'get_youtube_transcript' function\n",
        "    transcript = get_youtube_transcript(video_url)\n",
        "\n",
        "    # 3. Check if transcript was fetched successfully\n",
        "    if not transcript:\n",
        "        print(\"\\nCould not process this video. Please try another one with available English captions.\")\n",
        "        return\n",
        "\n",
        "    # 4. Build the RAG pipeline for this specific transcript\n",
        "    # We use our existing 'build_rag_pipeline' function\n",
        "    # This will set the global 'vector_db' variable for this session\n",
        "    build_rag_pipeline(transcript)\n",
        "\n",
        "    # 5. Start the interactive Q&A session\n",
        "    # We call our existing 'lecture_mitra_qa_session' function\n",
        "    lecture_mitra_qa_session()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lis7XnC5gyg8"
      },
      "source": [
        "# Step 7: The Main Application Orchestrator\n",
        "\n",
        "The start_lecturemitra function acts as the conductor for the entire orchestra. It defines the complete user journey from start to finish:\n",
        "\n",
        "It first prompts the user to enter a YouTube URL.\n",
        "It calls the transcript and RAG pipeline functions to set everything up.\n",
        "Finally, it launches the interactive chat session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGsiIVTP-oha"
      },
      "outputs": [],
      "source": [
        "import time #Import the time library#\n",
        "\n",
        "def lecture_mitra_qa_session():\n",
        "    \"\"\"\n",
        "    The main interactive loop for the LectureMitra tutor.\n",
        "    This corrected version includes a small delay to prevent the input prompt from breaking.\n",
        "    \"\"\"\n",
        "    global vector_db\n",
        "    if vector_db is None:\n",
        "        print(\"The RAG pipeline has not been built yet. Please run the setup cells first.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n--- Welcome to the LectureMitra QA Session! ---\")\n",
        "    print(\"You can ask questions about the video. Type 'exit' to end the session.\")\n",
        "    print(\"To ask a question with your voice, type 'voice' and press Enter.\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"\\nYou (or type 'voice'): \")\n",
        "\n",
        "        if user_input.lower() == 'exit':\n",
        "            print(\"Session ended. Goodbye!\")\n",
        "            break\n",
        "\n",
        "        user_question = \"\"\n",
        "        if user_input.lower() == 'voice':\n",
        "            audio_file = record_audio(seconds=5)\n",
        "            if os.path.exists(audio_file):\n",
        "                transcribed_question = transcribe_audio_with_sarvam(audio_file)\n",
        "                if transcribed_question and \"error\" not in transcribed_question.lower():\n",
        "                    user_question = transcribed_question\n",
        "                    print(f\"You (via voice): {user_question}\")\n",
        "                else:\n",
        "                    print(f\"Could not understand audio. Result: {transcribed_question}\")\n",
        "                    continue\n",
        "            else:\n",
        "                continue\n",
        "        else:\n",
        "            user_question = user_input\n",
        "\n",
        "        # This part remains the same\n",
        "        answer_text = ask_lecturemitra(user_question)\n",
        "        print(f\"\\nLectureMitra: {answer_text}\")\n",
        "        generate_and_play_speech(answer_text)\n",
        "\n",
        "        # Add a small delay to allow the output stream to recover #\n",
        "        time.sleep(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uR8IL5yJFY9p"
      },
      "source": [
        "#  Let's Get Started!\n",
        "\n",
        "All the components have been defined. Running the final cell below will start the LectureMitra application.\n",
        "\n",
        "What to Expect:\n",
        "\n",
        "The application will prompt you to enter a YouTube URL.\n",
        "\n",
        "Paste a link and press Enter. **Make sure the link you paste here is the shareable youtube video link (not the plain URL from browser)**\n",
        "\n",
        "After a few moments of processing, the Q&A session will begin. You can ask questions with text or by typing voice and speaking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X51K8gS4AsPX"
      },
      "outputs": [],
      "source": [
        "start_lecturemitra()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
